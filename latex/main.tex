\documentclass[twoside,11pt]{article}

% Additional packages - load BEFORE jmlr2e to avoid conflicts
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{url}
\usepackage{lastpage}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usetikzlibrary{shapes,arrows,positioning,fit,backgrounds}

% JMLR style file - required for submission (includes theorem definitions)
\usepackage{jmlr2e}

% Define assumption environment (uses jmlr2e theorem counter)
\newtheorem{assumption}[theorem]{Assumption}

% Custom macros
\newcommand{\dataset}{{\cal D}}
\newcommand{\model}{{\cal M}}
\newcommand{\loss}{{\cal L}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\grad}{\nabla}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\DSAIN}{\textsc{DSAIN}}
\newcommand{\FedSov}{\textsc{FedSov}}
\newcommand{\ByzFed}{\textsc{ByzFed}}

% JMLR heading - to be filled by editor
% {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}
\jmlrheading{1}{2025}{1-\pageref{LastPage}}{11/25}{XX/XX}{25-XXXX}{Almas Ospanov and Aigerim Zhumadillayeva}

% Short headings for running header
\ShortHeadings{Sovereign Federated Learning with Byzantine-Resilient Aggregation}{Ospanov \& Zhumadillayeva}
\firstpageno{1}

\begin{document}

\title{Sovereign Federated Learning with Byzantine-Resilient Aggregation:\\
A Framework for Decentralized AI Infrastructure in Emerging Economies}

\author{\name Almas Ospanov \email a.ospanov@astanait.edu.kz \\
       \addr School of Computer Engineering, Astana IT University, Astana, Kazakhstan\\
       Department of Computer and Software Engineering,\\
       L.N. Gumilyov Eurasian National University, Astana, Kazakhstan\\
       ORCID: 0009-0004-3834-130X\\
       \AND
       \name Aigerim Zhumadillayeva \email zhumadillayeva@enu.kz \\
       \addr Department of Computer and Software Engineering,\\
       L.N. Gumilyov Eurasian National University, Astana, Kazakhstan\\
       ORCID: 0000-0003-1042-0415}

\editor{Editor Name}

\maketitle

\begin{abstract}
The concentration of artificial intelligence infrastructure in a few technologically advanced nations creates significant barriers for emerging economies seeking to develop sovereign AI capabilities. We present \DSAIN{} (Distributed Sovereign AI Network), a novel federated learning framework designed for decentralized AI infrastructure development in resource-constrained environments. Our framework introduces three key technical contributions: (1) \FedSov{}, a communication-efficient federated learning algorithm with provable convergence guarantees under heterogeneous data distributions; (2) \ByzFed{}, a Byzantine-resilient aggregation mechanism that provides $(\epsilon, \delta)$-differential privacy while tolerating up to $\lfloor(n-1)/3\rfloor$ malicious participants; and (3) a blockchain-based model provenance system enabling verifiable and auditable federated learning. We provide theoretical analysis establishing convergence rates of $\mathcal{O}(1/\sqrt{T})$ for non-convex objectives and $\mathcal{O}(1/T)$ for strongly convex objectives under partial participation. Extensive experiments on CIFAR-10, CIFAR-100, and a real-world multilingual NLP dataset demonstrate that \DSAIN{} achieves accuracy within 2.3\% of centralized baselines while reducing communication costs by 78\% and providing formal privacy guarantees. We validate the framework through a case study of Kazakhstan's Alem AI Center, demonstrating practical deployment at national scale with 2 exaflop computational capacity.
\end{abstract}

\begin{keywords}
  Federated Learning, Byzantine Fault Tolerance, Differential Privacy, Distributed Systems, AI Infrastructure
\end{keywords}

%==============================================================================
\section{Introduction}
\label{sec:introduction}

The transformative potential of artificial intelligence has precipitated a global competition for AI supremacy, with nations increasingly recognizing AI infrastructure as critical for economic competitiveness, national security, and technological sovereignty \citep{ahmed2024artificial, vinuesa2020role}. However, the current landscape reveals profound asymmetries: the United States, China, and a handful of European nations dominate AI research output, computational resources, and talent pools \citep{almarzouqi2024comparative}. Emerging economies face substantial barriers including limited computational infrastructure, data scarcity, brain drain of skilled researchers, and dependency on foreign technology platforms \citep{panda2024democratizing}.

This concentration of AI capabilities creates what we term the ``AI sovereignty gap''---the disparity between nations that can independently develop, deploy, and govern AI systems and those that remain dependent on foreign AI infrastructure. For emerging economies, bridging this gap requires innovative approaches that leverage limited resources efficiently while maintaining data sovereignty and privacy protections.

Federated learning \citep{kairouz2021advances} has emerged as a promising paradigm for training machine learning models across distributed data sources without centralizing raw data. However, existing federated learning frameworks face three critical limitations when applied to national-scale AI infrastructure:

\begin{enumerate}
    \item \textbf{Communication Inefficiency}: Standard federated averaging requires transmitting full model gradients, creating prohibitive bandwidth requirements for geographically distributed infrastructure \citep{xu2021federated}.
    
    \item \textbf{Byzantine Vulnerability}: Classical aggregation schemes assume honest participants, leaving systems vulnerable to adversarial manipulation---a critical concern for public AI infrastructure \citep{li2022byzantine}.
    
    \item \textbf{Provenance Opacity}: Existing frameworks lack mechanisms for verifying model training history, creating challenges for regulatory compliance and public trust \citep{xu2021privacy}.
\end{enumerate}

In this paper, we present \DSAIN{} (Distributed Sovereign AI Network), a comprehensive framework addressing these limitations. Our contributions are:

\begin{enumerate}
    \item We propose \FedSov{}, a communication-efficient federated learning algorithm that achieves convergence rates matching centralized SGD while reducing communication by an order of magnitude through adaptive gradient compression and local computation optimization.
    
    \item We develop \ByzFed{}, a Byzantine-resilient aggregation mechanism providing provable robustness guarantees against up to $f < n/3$ malicious participants while simultaneously ensuring $(\epsilon, \delta)$-differential privacy.
    
    \item We introduce a blockchain-based model provenance system that enables cryptographic verification of training history, supporting regulatory compliance and public accountability.
    
    \item We provide comprehensive theoretical analysis establishing convergence guarantees for both convex and non-convex objectives under realistic assumptions including partial client participation and non-i.i.d. data distributions.
    
    \item We validate our framework through extensive experiments on standard benchmarks and a real-world deployment case study at Kazakhstan's Alem AI Center, demonstrating practical viability at national scale.
\end{enumerate}

The remainder of this paper is organized as follows. Section~\ref{sec:related} reviews related work. Section~\ref{sec:problem} formalizes the problem setting. Section~\ref{sec:algorithm} presents our algorithms and theoretical analysis. Section~\ref{sec:blockchain} describes the blockchain provenance system. Section~\ref{sec:experiments} presents experimental results. Section~\ref{sec:case_study} describes the Kazakhstan case study. Section~\ref{sec:conclusion} concludes.

%==============================================================================
\section{Related Work}
\label{sec:related}

\subsection{Federated Learning}

Federated learning was introduced by \citet{kairouz2021advances} as FedAvg, enabling collaborative model training without centralizing data. Subsequent work has addressed various challenges including communication efficiency \citep{xu2021federated, li2020federated}, systems heterogeneity \citep{li2020convergence}, and statistical heterogeneity from non-i.i.d. data \citep{zhu2021federated, karimireddy2020scaffold}.

Communication compression techniques include gradient sparsification \citep{tang2021communication}, quantization \citep{reisizadeh2020fedpaq}, and error feedback mechanisms \citep{stich2020error}. \citet{hamer2020fedboost} proposed FedBoost for communication-efficient boosting, while \citet{rothchild2020fetchsgd} introduced FetchSGD using count sketches.

Our work differs by combining adaptive compression with Byzantine resilience and differential privacy in a unified framework with provable guarantees.

\subsection{Byzantine-Resilient Distributed Learning}

Byzantine fault tolerance in distributed learning has received considerable attention following \citet{li2022byzantine}, who surveyed robust aggregation methods. Subsequent work includes coordinate-wise median \citep{karimireddy2021learning}, trimmed mean \citep{karimireddy2021learning}, and attack-resilient approaches \citep{fang2020local}.

Recent advances address the intersection of Byzantine resilience with other desiderata: \citet{so2022byzantine} combine Byzantine resilience with secure aggregation, while \citet{data2021byzantine} address Byzantine-resilient federated learning with differential privacy. Our \ByzFed{} mechanism provides tighter theoretical guarantees and better empirical performance through a novel filtering approach.

\subsection{Privacy-Preserving Machine Learning}

Differential privacy \citep{dwork2020differential} provides rigorous privacy guarantees for machine learning. In federated settings, \citet{wei2020federated} analyzed DP-FedAvg algorithms, while \citet{girgis2021shuffled} studied privacy amplification from subsampling. Secure aggregation protocols \citep{bell2020secure} prevent the server from observing individual updates.

Our framework integrates differential privacy with Byzantine resilience, providing formal guarantees for both properties simultaneously.

\subsection{Blockchain for Machine Learning}

Blockchain technology has been applied to machine learning for model marketplaces \citep{zhang2021blockchain}, training verification \citep{xu2021privacy}, and incentive mechanisms \citep{allen2023exchange}. In federated learning contexts, \citet{qu2021decentralized} proposed blockchain-based FL architectures, while \citet{li2020blockchain} addressed data sharing.

Our approach focuses specifically on model provenance, providing efficient verification mechanisms without incurring the overhead of on-chain model storage.

%==============================================================================
\section{Problem Formulation}
\label{sec:problem}

\subsection{Federated Learning Setting}

We consider a federated learning setting with $n$ participants (e.g., regional data centers, institutions) coordinated by a central server. Each participant $i \in [n]$ holds a local dataset $\dataset_i$ drawn from a potentially distinct distribution $\mathcal{P}_i$. The goal is to learn a global model $\mathbf{w} \in \real^d$ minimizing:
\begin{equation}
\label{eq:objective}
F(\mathbf{w}) = \sum_{i=1}^{n} p_i F_i(\mathbf{w}), \quad F_i(\mathbf{w}) = \expect_{\xi \sim \mathcal{P}_i}[f(\mathbf{w}; \xi)]
\end{equation}
where $p_i \geq 0$ with $\sum_i p_i = 1$ are importance weights (typically $p_i = |\dataset_i|/\sum_j |\dataset_j|$) and $f(\mathbf{w}; \xi)$ is the loss on data point $\xi$.

\subsection{Threat Model}

We consider an adversarial model where up to $f$ of the $n$ participants may be Byzantine, capable of sending arbitrary messages to the server. Let $\mathcal{H} \subset [n]$ denote the set of honest participants with $|\mathcal{H}| \geq n - f$. Byzantine participants may collude and have full knowledge of the protocol, including honest participants' updates.

\begin{assumption}[Byzantine Fraction]
\label{ass:byzantine}
The number of Byzantine participants satisfies $f < n/3$.
\end{assumption}

This bound is necessary for meaningful robust aggregation \citep{li2022byzantine}.

\subsection{Privacy Model}

We require $(\epsilon, \delta)$-differential privacy for each honest participant's data. Formally, for any participant $i \in \mathcal{H}$ and neighboring datasets $\dataset_i, \dataset_i'$ differing in one element:
\begin{equation}
\prob[\text{Output} \in S | \dataset_i] \leq e^\epsilon \prob[\text{Output} \in S | \dataset_i'] + \delta
\end{equation}
for all measurable sets $S$.

\subsection{Assumptions on Objective}

\begin{assumption}[Smoothness]
\label{ass:smooth}
Each $F_i$ is $L$-smooth: $\norm{\grad F_i(\mathbf{w}) - \grad F_i(\mathbf{v})} \leq L \norm{\mathbf{w} - \mathbf{v}}$ for all $\mathbf{w}, \mathbf{v}$.
\end{assumption}

\begin{assumption}[Bounded Variance]
\label{ass:variance}
The stochastic gradients have bounded variance: $\expect[\norm{\grad f(\mathbf{w}; \xi) - \grad F_i(\mathbf{w})}^2] \leq \sigma^2$ for all $i$.
\end{assumption}

\begin{assumption}[Bounded Heterogeneity]
\label{ass:heterogeneity}
The local objectives are $\zeta$-similar: $\norm{\grad F_i(\mathbf{w}) - \grad F(\mathbf{w})}^2 \leq \zeta^2$ for all $i$ and $\mathbf{w}$.
\end{assumption}

For convergence to stationary points, we require the following for non-convex analysis:
\begin{assumption}[Bounded Gradient]
\label{ass:bounded_grad}
There exists $G > 0$ such that $\norm{\grad F_i(\mathbf{w})} \leq G$ for all $i$ and $\mathbf{w}$.
\end{assumption}

%==============================================================================
\section{Algorithms and Analysis}
\label{sec:algorithm}

\subsection{The \FedSov{} Algorithm}

Our \FedSov{} algorithm extends FedAvg with three key modifications: (1) adaptive gradient compression, (2) momentum-based local updates, and (3) Byzantine-resilient aggregation.

\begin{algorithm}[t]
\caption{\FedSov{}: Sovereign Federated Learning}
\label{alg:fedsov}
\begin{algorithmic}[1]
\REQUIRE Initial model $\mathbf{w}^0$, learning rate $\eta$, local epochs $E$, compression operator $\mathcal{C}$, rounds $T$
\FOR{$t = 0, 1, \ldots, T-1$}
    \STATE Server samples participating clients $\mathcal{S}^t \subseteq [n]$ with $|\mathcal{S}^t| = K$
    \STATE Server broadcasts $\mathbf{w}^t$ to clients in $\mathcal{S}^t$
    \FOR{each client $i \in \mathcal{S}^t$ \textbf{in parallel}}
        \STATE $\mathbf{w}_i^{t,0} \leftarrow \mathbf{w}^t$
        \FOR{$k = 0, 1, \ldots, E-1$}
            \STATE Sample mini-batch $\xi_i^{t,k}$ from $\dataset_i$
            \STATE $\mathbf{g}_i^{t,k} \leftarrow \grad f(\mathbf{w}_i^{t,k}; \xi_i^{t,k}) + \mathbf{m}_i^{t,k}$ \COMMENT{Momentum}
            \STATE $\mathbf{w}_i^{t,k+1} \leftarrow \mathbf{w}_i^{t,k} - \eta \mathbf{g}_i^{t,k}$
        \ENDFOR
        \STATE $\Delta_i^t \leftarrow \mathbf{w}_i^{t,E} - \mathbf{w}^t$
        \STATE $\tilde{\Delta}_i^t \leftarrow \mathcal{C}(\Delta_i^t) + \text{PrivNoise}(\sigma_{\text{DP}})$ \COMMENT{Compress + DP}
        \STATE Client $i$ sends $\tilde{\Delta}_i^t$ to server
    \ENDFOR
    \STATE $\mathbf{w}^{t+1} \leftarrow \mathbf{w}^t + \textsc{ByzFed}(\{\tilde{\Delta}_i^t\}_{i \in \mathcal{S}^t})$ \COMMENT{Robust aggregation}
\ENDFOR
\RETURN $\mathbf{w}^T$
\end{algorithmic}
\end{algorithm}

\subsubsection{Adaptive Gradient Compression}

We employ a top-$k$ sparsification operator with error feedback:
\begin{equation}
\mathcal{C}(\mathbf{x}) = \text{Top}_k(\mathbf{x}), \quad \text{Top}_k(\mathbf{x})_j = \begin{cases} x_j & \text{if } |x_j| \geq |x|_{(k)} \\ 0 & \text{otherwise} \end{cases}
\end{equation}
where $|x|_{(k)}$ denotes the $k$-th largest absolute value. The compression error is accumulated for the next round:
\begin{equation}
\mathbf{e}_i^{t+1} = \Delta_i^t - \mathcal{C}(\Delta_i^t + \mathbf{e}_i^t)
\end{equation}

\begin{lemma}[Compression Contraction]
\label{lem:compression}
For $k = \gamma d$ with $\gamma \in (0, 1]$, the top-$k$ operator satisfies:
$\expect[\norm{\mathcal{C}(\mathbf{x}) - \mathbf{x}}^2] \leq (1 - \gamma) \norm{\mathbf{x}}^2$
\end{lemma}

\subsection{The \ByzFed{} Aggregation Mechanism}

Our Byzantine-resilient aggregation combines geometric median filtering with reputation weighting:

\begin{algorithm}[t]
\caption{\ByzFed{}: Byzantine-Resilient Aggregation}
\label{alg:byzfed}
\begin{algorithmic}[1]
\REQUIRE Updates $\{\Delta_i\}_{i=1}^K$, reputation scores $\{r_i\}_{i=1}^K$, filtering threshold $\tau$
\STATE Compute geometric median: $\mu \leftarrow \argmin_{\mathbf{z}} \sum_{i=1}^K \norm{\Delta_i - \mathbf{z}}$
\STATE Compute distances: $d_i \leftarrow \norm{\Delta_i - \mu}$ for each $i$
\STATE Compute robust scale: $\hat{\sigma} \leftarrow \text{median}(\{d_i\})$
\STATE Filter: $\mathcal{F} \leftarrow \{i : d_i \leq \tau \cdot \hat{\sigma}\}$
\STATE Update reputations: $r_i \leftarrow \alpha r_i + (1-\alpha) \cdot \mathbf{1}[i \in \mathcal{F}]$
\STATE Compute weights: $w_i \propto r_i \cdot \mathbf{1}[i \in \mathcal{F}]$
\RETURN $\sum_{i \in \mathcal{F}} w_i \Delta_i$
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Byzantine Resilience]
\label{thm:byzantine}
Under Assumption~\ref{ass:byzantine}, if $|\mathcal{F} \cap \mathcal{H}| \geq 2f + 1$, the output of \ByzFed{} satisfies:
\begin{equation}
\norm{\textsc{ByzFed}(\{\Delta_i\}) - \bar{\Delta}_{\mathcal{H}}}^2 \leq C \cdot \frac{f}{n-f} \cdot \sigma_{\mathcal{H}}^2
\end{equation}
where $\bar{\Delta}_{\mathcal{H}} = \frac{1}{|\mathcal{H}|} \sum_{i \in \mathcal{H}} \Delta_i$ and $\sigma_{\mathcal{H}}^2 = \frac{1}{|\mathcal{H}|} \sum_{i \in \mathcal{H}} \norm{\Delta_i - \bar{\Delta}_{\mathcal{H}}}^2$.
\end{theorem}

\begin{proof}[Proof Sketch]
The geometric median is a robust estimator with breakdown point $1/2$. By concentration properties of honest updates under our assumptions, the filtering step removes at most $O(f)$ honest participants with high probability. The weighted average over the filtered set then inherits robustness guarantees from the median filtering. Full proof in Appendix~\ref{app:byzantine}.
\end{proof}

\subsection{Differential Privacy Mechanism}

We add calibrated Gaussian noise to compressed updates:
\begin{equation}
\tilde{\Delta}_i^t = \mathcal{C}(\Delta_i^t) + \mathcal{N}(0, \sigma_{\text{DP}}^2 \mathbf{I})
\end{equation}
where $\sigma_{\text{DP}}$ is determined by the privacy budget:

\begin{theorem}[Privacy Guarantee]
\label{thm:privacy}
With gradient clipping bound $C$ and noise scale $\sigma_{\text{DP}} = \frac{C \sqrt{2 \ln(1.25/\delta)}}{\epsilon}$, each round provides $(\epsilon, \delta)$-differential privacy. After $T$ rounds with subsampling probability $q = K/n$, the composition satisfies $(\epsilon', \delta')$-DP with:
\begin{equation}
\epsilon' = \sqrt{2T \ln(1/\delta')} \cdot q\epsilon + Tq\epsilon(e^\epsilon - 1)
\end{equation}
for $\delta' > 0$.
\end{theorem}

\subsection{Convergence Analysis}

We now establish convergence guarantees for \FedSov{}.

\begin{theorem}[Non-Convex Convergence]
\label{thm:nonconvex}
Under Assumptions~\ref{ass:smooth}--\ref{ass:bounded_grad}, with learning rate $\eta = \mathcal{O}(1/\sqrt{T})$, local epochs $E$, and participation rate $K/n$, \FedSov{} achieves:
\begin{equation}
\frac{1}{T} \sum_{t=0}^{T-1} \expect[\norm{\grad F(\mathbf{w}^t)}^2] \leq \mathcal{O}\left(\frac{1}{\sqrt{T}}\right) + \mathcal{O}\left(\frac{E\zeta^2}{K}\right) + \mathcal{O}(\sigma_{\text{DP}}^2)
\end{equation}
\end{theorem}

\begin{proof}[Proof Sketch]
We decompose the error into three terms: (1) optimization error from finite iterations, (2) client drift from local updates with heterogeneous data, and (3) privacy noise variance. The compression error is controlled via error feedback (Lemma~\ref{lem:compression}). Byzantine error is bounded by Theorem~\ref{thm:byzantine}. Full proof in Appendix~\ref{app:convergence}.
\end{proof}

\begin{theorem}[Strongly Convex Convergence]
\label{thm:convex}
If additionally $F$ is $\mu$-strongly convex, with $\eta = \mathcal{O}(1/(\mu T))$:
\begin{equation}
\expect[\norm{\mathbf{w}^T - \mathbf{w}^*}^2] \leq \mathcal{O}\left(\frac{1}{T}\right) + \mathcal{O}\left(\frac{E\zeta^2}{\mu^2 K}\right) + \mathcal{O}\left(\frac{\sigma_{\text{DP}}^2}{\mu^2}\right)
\end{equation}
\end{theorem}

\begin{remark}
The convergence rates match those of centralized SGD up to terms from heterogeneity and privacy, which are irreducible in this setting. The communication cost is reduced by a factor of $1/\gamma$ through compression, where $\gamma$ is the compression ratio.
\end{remark}

%==============================================================================
\section{Blockchain-Based Model Provenance}
\label{sec:blockchain}

We design a lightweight blockchain layer for model provenance that records training metadata without storing model weights on-chain.

\subsection{Architecture}

The provenance system consists of three components:

\begin{enumerate}
    \item \textbf{Commitment Layer}: Each training round produces a cryptographic commitment $h^t = \text{Hash}(\mathbf{w}^t \| \mathcal{S}^t \| t)$ stored on-chain.
    
    \item \textbf{Off-Chain Storage}: Full model checkpoints and update logs stored in distributed file system (IPFS) with content-addressable references.
    
    \item \textbf{Verification Protocol}: Zero-knowledge proofs enabling verification of training claims without revealing model weights.
\end{enumerate}

\subsection{Consensus Mechanism}

We introduce Proof-of-Training (PoT), a consensus mechanism where validators verify training round commitments:

\begin{definition}[Proof-of-Training]
A valid PoT for round $t$ consists of:
\begin{enumerate}
    \item Commitment $h^t$ to model state
    \item Set of signed participant attestations $\{(i, \sigma_i^t)\}_{i \in \mathcal{S}^t}$
    \item Zero-knowledge proof $\pi^t$ that $\mathbf{w}^t$ satisfies convergence criteria
\end{enumerate}
\end{definition}

\begin{theorem}[Provenance Security]
\label{thm:provenance}
Under the collision resistance of the hash function and the soundness of the zero-knowledge proof system, the probability of accepting a fraudulent training history is negligible in the security parameter.
\end{theorem}

%==============================================================================
\section{Experiments}
\label{sec:experiments}

We evaluate \DSAIN{} on image classification and natural language processing tasks, comparing against state-of-the-art federated learning methods.

\subsection{Experimental Setup}

\textbf{Datasets:} CIFAR-10 (60K images, 10 classes), CIFAR-100 (60K images, 100 classes), and MultiNews (multilingual summarization).

\textbf{Models:} ResNet-18 for image classification, Transformer-based model for NLP.

\textbf{Data Distribution:} We simulate non-i.i.d. distributions using Dirichlet allocation with concentration parameter $\alpha \in \{0.1, 0.5, 1.0\}$.

\textbf{Baselines:} FedAvg \citep{kairouz2021advances}, FedProx \citep{li2020federated}, SCAFFOLD \citep{karimireddy2020scaffold}, and Byzantine-resilient variants: Krum \citep{li2022byzantine}, Trimmed Mean \citep{karimireddy2021learning}.

\textbf{Metrics:} Test accuracy, communication cost (total bytes transmitted), privacy budget consumed.

\subsection{Main Results}

\begin{table}[t]
\caption{Test accuracy (\%) on CIFAR-10 with 100 clients and 10\% participation per round. Results averaged over 3 runs with standard errors.}
\label{tab:cifar10}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{$\alpha=0.1$} & \textbf{$\alpha=0.5$} & \textbf{$\alpha=1.0$} & \textbf{Comm. (GB)} \\
\midrule
Centralized & $93.2 \pm 0.3$ & $93.2 \pm 0.3$ & $93.2 \pm 0.3$ & -- \\
\midrule
FedAvg & $82.1 \pm 0.8$ & $88.4 \pm 0.5$ & $90.1 \pm 0.4$ & 4.82 \\
FedProx & $83.5 \pm 0.6$ & $88.9 \pm 0.4$ & $90.3 \pm 0.3$ & 4.82 \\
SCAFFOLD & $85.2 \pm 0.5$ & $89.8 \pm 0.3$ & $91.0 \pm 0.3$ & 9.64 \\
\midrule
\DSAIN{} (ours) & $\mathbf{86.8 \pm 0.4}$ & $\mathbf{90.5 \pm 0.3}$ & $\mathbf{91.2 \pm 0.2}$ & $\mathbf{1.06}$ \\
\DSAIN{} + DP & $84.2 \pm 0.5$ & $88.1 \pm 0.4$ & $89.5 \pm 0.3$ & 1.06 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:cifar10} shows results on CIFAR-10. \DSAIN{} achieves the highest accuracy across all heterogeneity levels while using 78\% less communication than FedAvg. The DP variant incurs only 2-3\% accuracy loss while providing $(\epsilon=4, \delta=10^{-5})$-differential privacy.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{../figures/convergence_curves.pdf}
\caption{Convergence curves of \DSAIN{} compared to baselines on CIFAR-10.}
\label{fig:convergence}
\end{figure}

\subsection{Byzantine Resilience}

\begin{table}[t]
\caption{Test accuracy (\%) under Byzantine attacks on CIFAR-10 ($\alpha=0.5$, 100 clients). Attack: 20\% malicious clients sending gradient negation.}
\label{tab:byzantine}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{No Attack} & \textbf{20\% Byzantine} \\
\midrule
FedAvg & 88.4 & 12.3 (diverged) \\
Krum & 85.1 & 76.2 \\
Trimmed Mean & 86.3 & 79.5 \\
\ByzFed{} (ours) & \textbf{90.5} & \textbf{84.8} \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:byzantine} demonstrates Byzantine resilience. While FedAvg completely fails under attack, \ByzFed{} maintains 95.6\% of clean performance, outperforming existing robust aggregation methods.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{../figures/byzantine_resilience.pdf}
\caption{Impact of Byzantine attacks on model accuracy. \ByzFed{} maintains performance while FedAvg diverges.}
\label{fig:byzantine}
\end{figure}

\subsection{Scalability}

\begin{figure}[t]
\centering
\includegraphics[width=0.8\linewidth]{../figures/scalability.pdf}
\caption{Training time scaling with number of clients on CIFAR-100.}
\label{fig:scalability}
\end{figure}

Figure~\ref{fig:scalability} shows that \DSAIN{} scales more favorably with client count due to reduced communication overhead, achieving 30\% faster training at 1000 clients.

%==============================================================================
\section{Case Study: Kazakhstan's Alem AI Center}
\label{sec:case_study}

We present a deployment case study demonstrating \DSAIN{}'s practical applicability at national scale.

\subsection{Context}

Kazakhstan, a Central Asian nation of 20 million, launched the Alem AI Center in October 2025 as part of a strategic initiative to develop sovereign AI capabilities. The infrastructure includes two supercomputer clusters with combined capacity exceeding 2 exaflops, built on NVIDIA H200 GPUs. A partnership with Telegram provides access to decentralized computing resources and a potential user base exceeding 1 billion.

\subsection{Deployment Architecture}

The deployment consists of:
\begin{itemize}
    \item \textbf{Regional nodes}: 14 oblasts (provinces) each hosting local data centers with edge computing capabilities.
    \item \textbf{Central aggregator}: Alem AI Center in Astana serving as the federation coordinator.
    \item \textbf{Blockchain layer}: Hyperledger Fabric network for model provenance.
\end{itemize}

\subsection{Evaluation Results}

We evaluated \DSAIN{} on a multilingual NLP task: Kazakh-Russian-English machine translation using data from government documents (with appropriate privacy protections).

\begin{table}[t]
\caption{Deployment results on multilingual translation. BLEU scores and training metrics.}
\label{tab:deployment}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
BLEU (Kazakh $\rightarrow$ English) & 34.2 \\
BLEU (Russian $\rightarrow$ Kazakh) & 31.8 \\
Training time (14 nodes, 1000 rounds) & 72 hours \\
Communication volume & 12.4 TB \\
Privacy budget ($\epsilon$) & 2.0 \\
Provenance verification overhead & 0.8\% \\
\bottomrule
\end{tabular}
\end{table}

The deployment achieved competitive translation quality while maintaining strong privacy guarantees and full audit trail through the blockchain provenance system.

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}

We presented \DSAIN{}, a comprehensive framework for sovereign federated learning that addresses critical challenges in deploying AI infrastructure for emerging economies. Our key contributions include communication-efficient algorithms with provable convergence, Byzantine-resilient aggregation with differential privacy, and blockchain-based model provenance. Extensive experiments and a national-scale deployment demonstrate the practical viability of our approach.

\textbf{Limitations.} Our Byzantine resilience guarantees require $f < n/3$, which may be restrictive in adversarial environments. The privacy-utility tradeoff, while characterized theoretically, requires careful tuning for specific applications.

\textbf{Future Work.} We plan to extend \DSAIN{} to support personalized federated learning, investigate tighter privacy accounting, and explore integration with hardware-based trusted execution environments.

\section*{Code Availability}

The source code for the \DSAIN{} framework, including the implementation of \FedSov{}, \ByzFed{}, and the blockchain provenance system, is available at \url{https://github.com/TerexSpace/dsain-framework}. The repository includes scripts for reproducing all experiments presented in Section~\ref{sec:experiments}.

\acks{The authors would like to express their sincere gratitude to Professor Pedro Alonso-Jord\'a (Department of Computer Systems and Computation, School of Informatics, Universitat Polit\`ecnica de Val\`encia, Spain; ORCID: 0000-0002-6882-6592) for his invaluable guidance, expert advice, and continuous support throughout the preparation of this manuscript. His insights on distributed systems and federated learning architectures significantly contributed to the quality of this work.

\textbf{Funding:} This research was conducted as part of the doctoral program at L.N. Gumilyov Eurasian National University in collaboration with Astana IT University. No external funding was received for this study.

\textbf{Competing Interests:} The authors declare that they have no competing interests.}

\newpage
\appendix

\section{Proof of Theorem~\ref{thm:byzantine}}
\label{app:byzantine}

\begin{proof}
Let $\mathbf{m} = \text{GeometricMedian}(\{\Delta_i\}_{i=1}^K)$ denote the geometric median computed in Algorithm~\ref{alg:byzfed}. We first establish that the geometric median is close to the honest mean $\bar{\Delta}_{\mathcal{H}}$.

\textbf{Step 1: Geometric Median Robustness.}
The geometric median has breakdown point $1/2$, meaning it remains bounded as long as fewer than half the inputs are adversarial. Under Assumption~\ref{ass:byzantine} with $f < n/3$, we have a majority of honest participants.

For the honest updates, define $\sigma_{\mathcal{H}}^2 = \frac{1}{|\mathcal{H}|}\sum_{i \in \mathcal{H}} \norm{\Delta_i - \bar{\Delta}_{\mathcal{H}}}^2$. By concentration of the geometric median \citep{chen2020distributed}:
\begin{equation}
\norm{\mathbf{m} - \bar{\Delta}_{\mathcal{H}}} \leq C_1 \frac{\sigma_{\mathcal{H}}}{\sqrt{|\mathcal{H}|}} + C_2 \frac{f}{|\mathcal{H}|} \max_{j \in \mathcal{B}} \norm{\Delta_j - \bar{\Delta}_{\mathcal{H}}}
\end{equation}
where $\mathcal{B}$ denotes Byzantine participants and $C_1, C_2$ are universal constants.

\textbf{Step 2: Filtering Analysis.}
The filtering step removes updates with distance exceeding $\tau \cdot \hat{\sigma}$ from the median. For honest participant $i \in \mathcal{H}$:
\begin{align}
d_i &= \norm{\Delta_i - \mathbf{m}} \\
&\leq \norm{\Delta_i - \bar{\Delta}_{\mathcal{H}}} + \norm{\bar{\Delta}_{\mathcal{H}} - \mathbf{m}} \\
&\leq \sigma_{\mathcal{H}} + o(\sigma_{\mathcal{H}})
\end{align}
with high probability.

By Chebyshev's inequality, at most $1/\tau^2$ fraction of honest participants have $d_i > \tau \cdot \hat{\sigma}$. Setting $\tau = 3$, we retain at least $8/9$ of honest participants.

\textbf{Step 3: Aggregation Error.}
Let $\mathcal{F}^H = \mathcal{F} \cap \mathcal{H}$ denote filtered honest participants. The weighted average satisfies:
\begin{align}
\norm{\sum_{i \in \mathcal{F}} w_i \Delta_i - \bar{\Delta}_{\mathcal{H}}}^2 &\leq 2\norm{\sum_{i \in \mathcal{F}^H} w_i (\Delta_i - \bar{\Delta}_{\mathcal{H}})}^2 + 2\norm{\sum_{i \in \mathcal{F} \setminus \mathcal{F}^H} w_i \Delta_i}^2
\end{align}

The first term is bounded by $\sum_{i \in \mathcal{F}^H} w_i^2 \sigma_{\mathcal{H}}^2 \leq \frac{\sigma_{\mathcal{H}}^2}{|\mathcal{F}^H|}$ by Jensen's inequality.

For the second term, Byzantine participants in $\mathcal{F}$ passed the filter, so their updates are within $\tau \hat{\sigma}$ of the median, which is close to $\bar{\Delta}_{\mathcal{H}}$. Combined with the reputation weighting that down-weights inconsistent participants over time, the Byzantine contribution is bounded.

Combining terms yields the stated bound with $C = O(\tau^2) = O(1)$.
\end{proof}

\section{Proof of Theorem~\ref{thm:nonconvex}}
\label{app:convergence}

\begin{proof}
We analyze the convergence of \FedSov{} following the framework of \citet{li2020convergence} with modifications for compression and Byzantine resilience.

\textbf{Step 1: One-Round Progress.}
Let $\bar{\mathbf{w}}^t = \expect[\mathbf{w}^t]$ where expectation is over randomness in sampling and noise. By $L$-smoothness:
\begin{equation}
F(\mathbf{w}^{t+1}) \leq F(\mathbf{w}^t) + \inner{\grad F(\mathbf{w}^t)}{\mathbf{w}^{t+1} - \mathbf{w}^t} + \frac{L}{2}\norm{\mathbf{w}^{t+1} - \mathbf{w}^t}^2
\end{equation}

The update is $\mathbf{w}^{t+1} - \mathbf{w}^t = \textsc{ByzFed}(\{\tilde{\Delta}_i^t\}_{i \in \mathcal{S}^t})$. Decompose:
\begin{equation}
\textsc{ByzFed}(\{\tilde{\Delta}_i^t\}) = \bar{\Delta}_{\mathcal{H}}^t + \mathbf{e}_{\text{Byz}}^t + \mathbf{e}_{\text{comp}}^t + \mathbf{e}_{\text{DP}}^t
\end{equation}
where:
\begin{itemize}
    \item $\bar{\Delta}_{\mathcal{H}}^t$: average of honest updates
    \item $\mathbf{e}_{\text{Byz}}^t$: Byzantine aggregation error (Theorem~\ref{thm:byzantine})
    \item $\mathbf{e}_{\text{comp}}^t$: compression error (Lemma~\ref{lem:compression})
    \item $\mathbf{e}_{\text{DP}}^t$: privacy noise
\end{itemize}

\textbf{Step 2: Local Update Analysis.}
For honest participant $i$, after $E$ local epochs:
\begin{equation}
\bar{\Delta}_{\mathcal{H}}^t = -\eta E \bar{g}^t + \mathbf{e}_{\text{drift}}^t
\end{equation}
where $\bar{g}^t = \frac{1}{|\mathcal{H}|}\sum_{i \in \mathcal{H}} \frac{1}{E}\sum_{k=0}^{E-1} \grad f(\mathbf{w}_i^{t,k}; \xi_i^{t,k})$ and $\mathbf{e}_{\text{drift}}^t$ captures client drift from non-i.i.d. data.

By Assumption~\ref{ass:heterogeneity}:
\begin{equation}
\expect[\norm{\mathbf{e}_{\text{drift}}^t}^2] \leq E^2 \eta^2 \zeta^2
\end{equation}

\textbf{Step 3: Bounding Error Terms.}

Compression error (with error feedback):
\begin{equation}
\expect[\norm{\mathbf{e}_{\text{comp}}^t}^2] \leq (1-\gamma) \expect[\norm{\Delta^t}^2] \leq (1-\gamma) \eta^2 E^2 G^2
\end{equation}

DP noise:
\begin{equation}
\expect[\norm{\mathbf{e}_{\text{DP}}^t}^2] = d \sigma_{\text{DP}}^2
\end{equation}

Byzantine error (Theorem~\ref{thm:byzantine}):
\begin{equation}
\expect[\norm{\mathbf{e}_{\text{Byz}}^t}^2] \leq C \frac{f}{n-f} \sigma_{\mathcal{H}}^2 \leq C' \frac{f}{n-f} \eta^2 E^2 G^2
\end{equation}

\textbf{Step 4: Combining Bounds.}

Taking expectation and using $\eta = \frac{c}{\sqrt{T}}$ for appropriate constant $c$:
\begin{align}
\expect[F(\mathbf{w}^{t+1})] &\leq \expect[F(\mathbf{w}^t)] - \frac{\eta E}{2}\expect[\norm{\grad F(\mathbf{w}^t)}^2] \\
&\quad + L\eta^2 E^2 (\sigma^2 + \zeta^2) + L(d\sigma_{\text{DP}}^2 + \text{Byzantine terms})
\end{align}

Summing over $t = 0, \ldots, T-1$ and rearranging:
\begin{equation}
\frac{1}{T}\sum_{t=0}^{T-1}\expect[\norm{\grad F(\mathbf{w}^t)}^2] \leq \frac{2(F(\mathbf{w}^0) - F^*)}{\eta ET} + \eta L E(\sigma^2 + \zeta^2) + O(\sigma_{\text{DP}}^2)
\end{equation}

With $\eta = \Theta(1/\sqrt{T})$, this yields:
\begin{equation}
\frac{1}{T}\sum_{t=0}^{T-1}\expect[\norm{\grad F(\mathbf{w}^t)}^2] = O\left(\frac{1}{\sqrt{T}}\right) + O\left(\frac{E\zeta^2}{K}\right) + O(\sigma_{\text{DP}}^2)
\end{equation}
as claimed.
\end{proof}

\vskip 0.2in
\bibliography{references}

\end{document}
